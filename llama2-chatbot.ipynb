{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1Fzeo7b-nBxfEXejcbaCbEwMYNMIpaoUR","authorship_tag":"ABX9TyN5OUF72M2DxVM6kTZhdcab"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Prepare Prompt data for fine tuning\n"],"metadata":{"id":"5_jkHYLcE7Ay"}},{"cell_type":"code","source":["!pip install datasets\n"],"metadata":{"id":"Z_ywOCDGFa2j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset"],"metadata":{"id":"fApbDvGuFRA8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = load_dataset('csv', data_files={'train': 'train.csv', 'validation':'validation.csv', 'test': 'test.csv'})\n","dataset"],"metadata":{"id":"3xWM2K9JB7S1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Shuffle the dataset and slice it\n","dataset['train'] = dataset['train'].shuffle(seed=42)\n","\n","# Define a function to transform the data\n","def transform_conversation(example):\n","    prompt = 'If you are a licensed psychologist, please provide this client with a helpful response to her concern.'\n","    formatted_prompt = f\"<s>[INST] <<SYS>> {prompt} <</SYS>> {example['Question']} [/INST] {example['Answer']} </s>\"\n","\n","    return {'Prompt': formatted_prompt}\n","\n","\n","# Apply the transformation\n","transformed_dataset = dataset.map(transform_conversation)"],"metadata":{"id":"8lFsC1vQGB8s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset['train'][0]\n","transformed_dataset['train'][0]"],"metadata":{"id":"5P_n9ES-JuAx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformed_dataset.shape"],"metadata":{"id":"CofefOolyZ-0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install \\\n","    torch==2.0.1 --quiet \\\n","    transformers==4.31.0 \\\n","    datasets==2.11.0 \\\n","    evaluate==0.4.0 \\\n","    accelerate==0.21.0 \\\n","    loralib==0.1.1 \\\n","    peft==0.4.0 \\\n","    bitsandbytes==0.40.2 \\\n","    trl==0.4.7"],"metadata":{"id":"ihuhBMKw7mnc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"na7Uvpo5-h-N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel, AutoPeftModelForCausalLM\n","from trl import SFTTrainer"],"metadata":{"id":"QivTaAAe7icM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0PejjPpn6ZYd"},"outputs":[],"source":["# The model that you want to train from the Hugging Face hub\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n","\n","# Fine-tuned model name\n","new_model = \"llama-2-7b-therapy\"\n","peft_model_path = './drive/MyDrive/aitherapy/peft'\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# LoRA attention dimension\n","lora_r = 64\n","\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./drive/MyDrive/aitherapy/results\"\n","\n","# Number of training epochs\n","num_train_epochs = 1\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule (constant a bit better than cosine)\n","lr_scheduler_type = \"constant\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 500\n","\n","# Log every X updates steps\n","logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = None\n","\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}"]},{"cell_type":"code","source":["# Load tokenizer and model with QLoRA configuration\n","check_point_path = \"./drive/MyDrive/aitherapy/results/Llama-2-7b-chat-hf/\"\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=transformed_dataset['train'],\n","    peft_config=peft_config,\n","    dataset_text_field=\"Prompt\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","# Train model\n","# trainer.train()\n","\n","# Save trained model\n","# trainer.model.save_pretrained(new_model)"],"metadata":{"id":"FA_VdYvE7g9F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ignore warnings\n","logging.set_verbosity(logging.CRITICAL)\n","\n","# Run text generation pipeline with our next model\n","prompt = \"What is a large language model?\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"],"metadata":{"id":"VkYDPoo-7wYE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["del model\n","del tokenizer\n","del trainer\n","del dataset\n","torch.cuda.empty_cache()"],"metadata":{"id":"E6S0zvG-QkR7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reload model in FP16 and merge it with LoRA weights\n","fine_tuned_model = PeftModel.from_pretrained(\n","    model,\n","    check_point_path,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n","    is_trainable=False\n",")\n","#model = model.merge_and_unload()\n","\n","# Reload tokenizer to save it\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""],"metadata":{"id":"Kaee_9Xz7zXw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r9mkmfXwYemD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run text generation pipeline with our next model\n","prompt = 'If you are a licensed psychologist, please provide this client with a helpful response to her concern.'\n","user_prompt = \"I feel stressed at work\"\n","formatted_prompt = f\"<s>[INST] <<SYS>> {prompt} <</SYS>> {user_prompt} [/INST]\"\n","\n","pipe = pipeline(task=\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer, max_length=200)\n","result = pipe(formatted_prompt)\n","print(result[0]['generated_text'])"],"metadata":{"id":"0E-lznKDwqGi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(result[0]['generated_text'])"],"metadata":{"id":"PVvZSG1ZHOD3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gradio\n","import gradio as gr"],"metadata":{"id":"LdlN_gF9Pfie"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def format_chat_prompt(message, chat_history):\n","    sys_prompt = \"Suppose you are a licensed psychologist, please provide this client with a helpful response to her concern.\\n\"\n","    prompts = []\n","    prompt = ''\n","    prompts.append(f\"<s>[INST] <<SYS>> {sys_prompt} <</SYS>>\")\n","    for turn in chat_history:\n","        user_message, bot_message = turn\n","        prompts.append(f\" {user_message} [/INST] {bot_message} </s>\")\n","    prompts.append(f\"{prompt}\\n<s>[INST] <<SYS>> {sys_prompt} <</SYS>> {message} [/INST]\")\n","    prompt = \"\\n\".join(prompts)\n","    return prompt\n","\n","def respond(message, chat_history):\n","        formatted_prompt = format_chat_prompt(message, chat_history)\n","        pipe = pipeline(task=\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer, max_length=200)\n","        result = pipe(formatted_prompt)\n","        print(result[0]['generated_text'])\n","        bot_message = result[0]['generated_text']\n","\n","        chat_history.append((message, bot_message))\n","        return \"\", chat_history\n","\n","with gr.Blocks() as demo:\n","    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n","    msg = gr.Textbox(label=\"Prompt\")\n","    btn = gr.Button(\"Submit\")\n","    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n","\n","    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n","    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n","gr.close_all()\n","demo.launch(debug=True, share=True)"],"metadata":{"id":"ucl_Z8u6P0I0"},"execution_count":null,"outputs":[]}]}